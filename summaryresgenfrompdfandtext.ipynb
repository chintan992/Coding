{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summaryresgenfrompdfandtext.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chintan992/Coding/blob/master/summaryresgenfrompdfandtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUrWi8SLk1Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "14e55327-3303-40c7-f5c0-e6a4707a9a8b"
      },
      "source": [
        "# importing required modules \n",
        "import PyPDF2 \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "  \n",
        "# creating a pdf file object \n",
        "pdfFileObj = open('keya1.pdf', 'rb') \n",
        "  \n",
        "# creating a pdf reader object \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "  \n",
        "# printing number of pages in pdf file \n",
        "print(pdfReader.numPages) \n",
        "  \n",
        "# creating a page object \n",
        "pageObj = pdfReader.getPage(0) \n",
        "  \n",
        "# extracting text from page \n",
        "text_str = pageObj.extractText()\n",
        "  \n",
        "  \n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we should only use the words that are not part of the stopWords array.\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its words\n",
        "    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
        "        word_count_in_sentence_except_stop_words = 0\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                word_count_in_sentence_except_stop_words += 1\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        if sentence[:10] in sentenceValue:\n",
        "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
        "\n",
        "        '''\n",
        "        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n",
        "        To solve this, we're dividing every sentence score by the number of words in the sentence.\n",
        "        \n",
        "        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n",
        "        the dictionary.\n",
        "        '''\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def run_summarization(text):\n",
        "    # 1 Create the word frequency table\n",
        "    freq_table = _create_frequency_table(text)\n",
        "\n",
        "    '''\n",
        "    We already have a sentence tokenizer, so we just need \n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "    '''\n",
        "\n",
        "    # 2 Tokenize the sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # 3 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(sentences, freq_table)\n",
        "\n",
        "    # 4 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "\n",
        "    # 5 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = run_summarization(text_str)\n",
        "    print(result)  \n",
        "    # closing the pdf file object \n",
        "    pdfFileObj.close() \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            " Particularly, information extraction is done when the \n",
            "data is unstructured or semi\n",
            "-\n",
            "structured in nature. This data c\n",
            "an be\n",
            " \n",
            "collected from the Internet\n",
            ". Large\n",
            "-\n",
            "scale\n",
            " \n",
            "data analysis\n",
            " \n",
            "is run\n",
            " \n",
            "in parallel on data \n",
            "stored in DFS. [7]\n",
            ". In the final section, the \n",
            "analysis and discussion is \n",
            "stated\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2tRTZtZSKKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "abee36a8-4676-4db0-8596-722b0c799b1a"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anCpnWLXpoc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "ca5f977a-15a2-48e0-ba39-3e0e94d73d54"
      },
      "source": [
        "# Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text_str = '''\n",
        "\n",
        "Big data phenomenon is a concept for large, heterogeneous and complex data sets and having many\n",
        "challenges in storing, preparing, analyzing and visualizing as well as techniques and technologies for making\n",
        "better decision and services. Uncovered hidden patterns, unknown or unpredicted relations and secret\n",
        "correlations are achieved via big data analytics. This might help companies and organizations to have new\n",
        "ideas, get richer and deeper insights, broaden their horizons, get advantages over their competitors, etc. To\n",
        "make big data analytics easy and efficient, a lot of big data techniques and technologies have been developed.\n",
        "In this article, the chronological development of batch, real-time and hybrid technologies, their advantages\n",
        "and disadvantages have been reviewed. A number of criticism have been focused on available processing\n",
        "techniques and technologies. This paper will be a roadmap for researchers who work on big data analytics.\n",
        "Keywords: big data, processing, technique, technology, tools, evaluations\n",
        "1. INTRODUCTION\n",
        "The size of data starts from giga to zetta bytes and\n",
        "beyond. According to Fortune1000 Companies, 10%\n",
        "of increase in data provides $65.7 million extra\n",
        "income (McCafferty, 2014). Big data flows too fast,\n",
        "requires too many new techniques, technologies,\n",
        "approaches and handles with the difficulties it brings.\n",
        "Big data is generated from online and offline\n",
        "processes, logs, transactions, click streams, emails,\n",
        "social network interactions, videos, audios, images,\n",
        "posts, books, photos, search queries, health records,\n",
        "science data, sensors, and mobile phones including\n",
        "their applications and traffics. They are stored in\n",
        "databases or clouds and the size of them continues to\n",
        "grow massively. As a result, it becomes difficult to\n",
        "capture, store, share, analyze and visualize the data\n",
        "with typical tools. Big data concepts have a\n",
        "combination of techniques and technologies that help\n",
        "experts, managers, directors, investors, companies\n",
        "and institutions to gain deeper insights into their\n",
        "information assets and also to abstract new ideas,\n",
        "ways, approaches, values, perceptions from the\n",
        "analyzed data (Dumbill, 2012). To enable an efficient\n",
        "decision making practice, organizations need\n",
        "\n",
        "effective processes to turn high volumes of fastmoving and diverse data into meaningful outcomes.\n",
        "The value of big data market is 10,2 billion dollars\n",
        "now, and it is expected to reach 53.4 billion dollars\n",
        "by 2017 (McCafferty, 2014). Organizations and\n",
        "institutions might get benefits from big data analysis\n",
        "for their future developments, investments, decisions,\n",
        "challenges, and directions with descriptive,\n",
        "predictive, and prescriptive analytics like decision\n",
        "support systems, personalized systems, user behavior\n",
        "analysis, market analysis, location-based services,\n",
        "social analysis, healthcare systems and scientific\n",
        "researches.\n",
        "To clarify and express the big data features, the\n",
        "five Vs of volume, variety, velocity, veracity and\n",
        "value (Dumbill, 2012), (Demchenko, Grosso, De\n",
        "Laat, & Membrey, 2013), (M. Chen, Mao, & Liu,\n",
        "2014) are frequently used to explain or understand\n",
        "the nature of big data (Fig. 1). Volume is the size of\n",
        "data produced or generated. It is huge and its size\n",
        "might be in terabytes, petabytes, exabytes or more.\n",
        "The volume is important to distinguish the big data\n",
        "from others. Variety has different forms of data,\n",
        "covers the complexity of big data and imposes new\n",
        "requirements in terms of analysts, technologies and\n",
        "tools. Big data is connected with variety of sources in\n",
        "three types: structured, semi structured and\n",
        "Services Transactions on Big Data (ISSN 2326-442X)\n",
        "unstructured. Velocity is important not only for big\n",
        "data but also for all processes. The speed of\n",
        "generating or processing big data is crucial for further\n",
        "steps to meet the demands and requirements.\n",
        "Veracity deals with consistency and trustworthy of\n",
        "big data. Recent statistics have shown that 1 of 3\n",
        "decision makers do not trust the information gathered\n",
        "from big data because of their inaccuracy (Center,\n",
        "2012). Accordingly, collected or analyzed big data\n",
        "should be in trusted origin, protected from\n",
        "unauthorized access and normal format, even if this is\n",
        "hard to achieve. Value is the most important feature\n",
        "of big data and provides outputs for the demands by\n",
        "business requirements. Accessing and analyzing big\n",
        "data is very important, but it is useless if no value is\n",
        "derived from this process. Values should be in\n",
        "different forms such as having statistical reports,\n",
        "realizing a trend that was invisible, finding cost\n",
        "saving resolutions, detecting improvements or\n",
        "considering new thoughts for better solutions or\n",
        "achievements.\n",
        "2. TECHNIQUES AND TECHNOLOGIES\n",
        "FOR BIG DATA\n",
        "Big data is a way of understanding not only the\n",
        "nature of data but also the relationships among data.\n",
        "Identifying characteristics of the data is helpful in\n",
        "defining its patterns. Key characteristics for big data\n",
        "are grouped into ten classes (Hashem et al., 2015),\n",
        "(Mysore & Jain, 2013), (Assunção, Calheiros,\n",
        "Bianchi, Netto, & Buyya, 2015) (Fig. 2).\n",
        "To\n",
        "enable\n",
        "efficient\n",
        "decision-making,\n",
        "organizations need effective processes to turn high\n",
        "volumes of fast moving and diverse data into\n",
        "meaningful outcomes. Big data analytics helps boost\n",
        "digital economy, and provide opportunities via\n",
        "supporting or replacing decision-making processes\n",
        "with automated algorithms. In addition to that, it\n",
        "helps reducing the cost and predicting behaviors of\n",
        "groups, teams, supporters, enemies or habits from\n",
        "enough features of available data. Data management\n",
        "of big data involves processes and supporting\n",
        "technologies to acquire, store and prepare data, while\n",
        "analytics refers to techniques used in analyzing and\n",
        "extracting intelligence from big data (Gandomi &\n",
        "Haider, 2015).\n",
        "The techniques for big data analytics consist of\n",
        "multiple disciplines including mathematics, statistics,\n",
        "data mining, pattern recognition, machine learning,\n",
        "signal processing, simulation, natural language\n",
        "processing, time series analysis, social network\n",
        "analysis, crowdsourcing, optimization methods, and\n",
        "visualization approaches (M. Chen et al., 2014). Big\n",
        "data analytics need new techniques to process huge\n",
        "amount of data in an efficient time manner and way\n",
        "to have better decisions and values.\n",
        "\n",
        "Figure 1. 5 V’s of big data\n",
        "Working with big data is a complex process with\n",
        "conceptual and technical challenges. This causes the\n",
        "existence of a high number of different approaches.\n",
        "In this paper, an overview on big data's concepts are\n",
        "summarized and techniques, technologies, tools and\n",
        "platforms for big data are generally reviewed in\n",
        "Section 1 and 2. In Section 3, the chronological\n",
        "development of big data processing is reviewed\n",
        "according to the technologies they cover. Finally,\n",
        "discussion and conclusion are outlined in Section 4.\n",
        "\n",
        "The technologies for big data processing\n",
        "paradigms are chronologically transformed as batch\n",
        "processing, real-time processing and hybrid\n",
        "computation because of the big data evolution\n",
        "(Casado & Younas, 2015). Batch processing is a\n",
        "solution for volume issue, real-time processing deals\n",
        "with velocity issue and hybrid computation is suitable\n",
        "for the both issues. The techniques and technologies\n",
        "developed in this context are summarized in Table 1\n",
        "as platforms, databases and tools (Wang & Chen,\n",
        "2013), (Cattell, 2011), (Bajpeyee, Sinha, & Kumar,\n",
        "2015). These tables should be helpful to companies,\n",
        "institutions or applicants to understand and provide\n",
        "new ideas, deep insights, perceptions and knowledge\n",
        "after analyzing big data.\n",
        "\n",
        "Data Usage\n",
        "\n",
        "Analysis Type\n",
        "\n",
        "Figure 2. Big data classification\n",
        "\n",
        "TOOLS\n",
        "Hadoop, Spark, MapR, Cloudera, Hortonworks, InfoSphere, IBM BigInsights, Asterix\n",
        "AWS EMR, Google Compute Engine, Microsoft Azure, Pure System, LexisNexis HPCC\n",
        "Systems\n",
        "\n",
        "DATABASE TYPE\n",
        "SQL\n",
        "Column\n",
        "\n",
        "TOOLS\n",
        "Greenplum, Aster Data, Vertica, SpliceMachine\n",
        "HBase, HadoopDB, Cassandra, Hypertable, BigTable, PNUTS, Cloudera, MonetDB,\n",
        "Accumulo, BangDB\n",
        "Redis, Flare, Sclaris, MemcacheDB, Hypertable, Valdemort, Hibari, Riak, BerkeleyDB,\n",
        "DynamoDB, Tokyo Cabinet, HamsterDB\n",
        "SimpleDB, RavenDB, ArangoDB MongoDB, Terrastore, CouchDB, Solr, Apache\n",
        "Jackrabbit, BaseX, OrientDB, FatDB, DjonDB\n",
        "Neo4J, InfoGrid, Infinite Graph, OpenLink, FlockDB, Meronymy, AllegroGraph, WhiteDB,\n",
        "TITAN, Trinity\n",
        "SAP HANA\n",
        "\n",
        "NOSQL\n",
        "\n",
        "PLATFORM TYPE\n",
        "LOCAL\n",
        "CLOUD\n",
        "\n",
        "Key-value\n",
        "Document\n",
        "Graph\n",
        "\n",
        "IN-MEMORY\n",
        "\n",
        "TOOL FUNCTIONS\n",
        "DATA PROCESSING\n",
        "DATA WAREHOUSE\n",
        "DATA AGGREGATION & TRANSFER\n",
        "SEARCH\n",
        "QUERY LANGUAGE\n",
        "STATISTICS & MACHINE LEARNING\n",
        "BUSINESS INTELLIGENCE\n",
        "VISUALIZATION\n",
        "SOCIAL MEDIA\n",
        "\n",
        "TOOLS\n",
        "MapReduce, Dryad, YARN, Storm, S4, BigQuery, Pig, Impala, Hive,\n",
        "Flink, Spark, Samza, Heron\n",
        "Hive, HadoopDB, Hadapt\n",
        "Sqoop, Flume, Chukwa, Kafka, ActiveMQ\n",
        "Lucene, Solr, ElasticSearch\n",
        "Pig Latin, HiveQL, DryadLINQ, MRQL, SCOPE, ECL, Impala\n",
        "Mahout, Weka, R, SAS, SPSS, Pyhton, Pig, RapidMiner, Orange, BigML,\n",
        "Skytree, SAMOA, Spark MLLib, H2O,\n",
        "Talend, Jaspersoft, Pentaho, KNIME\n",
        "Google Charts, Fusion Charts, Tableau Software, QlikView\n",
        "Radian6, Clarabridge\n",
        "\n",
        "Table 1. Big data tools in different perspectives\n",
        "\n",
        "Services Transactions on Big Data (ISSN 2326-442X)\n",
        "\n",
        "3. BIG DATA PROCESSING\n",
        "3.1 BATCH PROCESSING\n",
        "Big data batch processing was started with\n",
        "Google File System which is a distributed file system\n",
        "and MapReduce programming framework for\n",
        "distributed computing (Casado & Younas, 2015).\n",
        "MapReduce splits a complex problem into subproblems implemented by Map and Reduce steps.\n",
        "Complex big data problems are solved in parallel\n",
        "ways then combined the solution of original problem.\n",
        "Apache Hadoop is well-known big data platform\n",
        "consisting of Hadoop kernel, MapReduce and HDFS\n",
        "(Hadoop Distributed File System) besides a number\n",
        "of related projects, including Cassandra, Hive,\n",
        "HBase, Mahout, Pig and so on (C. P. Chen & Zhang,\n",
        "2014). The framework aims for distributed storage\n",
        "and processing of big data sets in clusters (P.\n",
        "Almeida, 2015). Microsoft Dryad is another\n",
        "programming model for implementing parallel and\n",
        "distributed programs that can scale up capability.\n",
        "Dryad executes operations on the vertexes in clusters\n",
        "and use channels for transmission of data. Dryad is\n",
        "not only more complex and powerful than\n",
        "Map/Reduce and the relational algebra but also\n",
        "support any amount of input and output data unlike\n",
        "MapReduce (M. Chen et al., 2014). HPCC (High\n",
        "Performance Computing Cluster) Systems are\n",
        "distributed data intensive open source computing\n",
        "platform and provide big data workflow management\n",
        "services. Unlike Hadoop, HPCC’s data model\n",
        "defined by user. The key to complex problems can be\n",
        "stated easily with high level ECL (Enterprise Control\n",
        "Language) basis. HPCC ensure that ECL is executed\n",
        "at the maximum elapsed time and nodes are\n",
        "processed in parallel. Furthermore, HPCC Platform\n",
        "does not require third party tools like GreenPlum,\n",
        "Oozie, Cassandra, RDBMS, etc. (\"Why HPCC\n",
        "Systems is a superior alternative to Hadoop,\").\n",
        "\n",
        "3.2 REAL-TIME PROCESSING\n",
        "Big Data Applications based on write once analyze\n",
        "multiple\n",
        "times\n",
        "data\n",
        "management\n",
        "architectures are unable to scale for real-time data\n",
        "operations. After some years from using MapReduce,\n",
        "big data analytic applications shifted to use Stream\n",
        "Processing paradigm (Tatbul, 2010). Hadoop-based\n",
        "programming models and frameworks are unable to\n",
        "\n",
        "Vol. 3, No. 1, 2016\n",
        "\n",
        "47\n",
        "\n",
        "offer the combination of latency and throughput\n",
        "requirements for real-time applications in industries\n",
        "such as real-time analytics, Internet of Things, fraud\n",
        "detection, system monitoring and cybersecurity.\n",
        "Stream processing programming model mainly\n",
        "depends on the freshness of the data in motion. When\n",
        "any type of data is generated at its source, processing\n",
        "the data from travelling from its source to its\n",
        "destination is very challenging and also effective\n",
        "way. Potential of this approach is very important\n",
        "because eliminating the latency for gaining value\n",
        "from the data has outstanding advantages. The data is\n",
        "analyzed to obtain results at once. The data travels\n",
        "from its source to destination as continuous or\n",
        "discrete streams and this time velocity property of the\n",
        "Big Data has to be handled with this approach.\n",
        "Using stream processing techniques in Big Data\n",
        "requires special frameworks to analyze and obtain\n",
        "value from the data. Because the data stream is fast\n",
        "and has a gigantic volume, solely a small part of the\n",
        "stream can be stored in bounded memory. In contrast\n",
        "to the batch data processing model where data is first\n",
        "stored, indexed and then processed by queries, stream\n",
        "processing gets the inbound data while it is in\n",
        "motion, as it streams through its target. Stream\n",
        "processing also connects to external data sources,\n",
        "providing applications to integrate selected data into\n",
        "the application flow, or to update a destination\n",
        "system with processed information.\n",
        "Despite not supporting stream processing,\n",
        "MapReduce can partially handle streams using\n",
        "micro-batching technique. The idea is to process the\n",
        "stream as a sequence of small data chunks. The\n",
        "incoming g stream is grouped to form a chunk of data\n",
        "and is sent to batch processing system to be\n",
        "processed in short intervals. Some MapReduce\n",
        "implementations especially real-time ones like Spark\n",
        "Streaming (Zaharia, Chowdhury, Franklin, Shenker,\n",
        "& Stoica, 2010) support this technique. However,\n",
        "this technique is not adequate for demands of a lowlatency stream processing system. In addition, the\n",
        "MapReduce model is not suitable for stream\n",
        "processing.\n",
        "The streaming processing paradigm is used for\n",
        "real time applications, generally at the second or even\n",
        "millisecond level. Typical open source stream\n",
        "processing frameworks Samza (Feng, Zhuang, Pan,\n",
        "& Ramachandra, 2015), Storm (Toshniwal et al.,\n",
        "2014), S4 (Neumeyer, Robbins, Nair, & Kesari,\n",
        "2010), and Flink (Renner, Thamsen, & Kao, 2015).\n",
        "They all are low-latency, distributed, scalable, faulttolerant and also provide simple APIs to abstract the\n",
        "complexity of the underlying implementations. Their\n",
        "\n",
        "\fServices Transactions on Big Data (ISSN 2326-442X)\n",
        "main approach is to process data streams through\n",
        "parallel tasks distributed across a computing cluster\n",
        "machines with fail-over capabilities.\n",
        "There are three general categories of delivery\n",
        "patterns for stream processing. These categories are:\n",
        "at-most-once, at-least-once and exactly-once. At most\n",
        "once delivery means that for each message handed to\n",
        "the next processing unit in a topology that message is\n",
        "delivered zero or one time, so there is a probability\n",
        "that messages may be lost during delivery. At least\n",
        "once delivery means that for each message handed to\n",
        "a processing unit in a topology potentially multiple\n",
        "endeavors are made for delivery, such that at least\n",
        "one time this operation is succeeded. Messages may\n",
        "be sent multiple times and duplication may occur but\n",
        "messages are not lost. Exactly once delivery means\n",
        "that for each message handed to a processing unit in a\n",
        "topology exactly once delivery is made to the\n",
        "receiver unit, so it prevents message lost and\n",
        "duplication.\n",
        "Another important point for stream processing\n",
        "frameworks is state management operations. There\n",
        "are different known strategies to store state. Spark\n",
        "Streaming writes state information into a storage.\n",
        "Samza uses an embedded key-value store. State\n",
        "management has to be handled either implementing\n",
        "at application level separately or using a higher-level\n",
        "abstraction, which is called Trident in Apache Storm.\n",
        "As for Flink, state management based on consistent\n",
        "global snapshots inspired Chandy-Lamport algorithm\n",
        "(Chandy & Lamport, 1985). It provides low runtime\n",
        "overhead and stateful exactly-once semantics.\n",
        "Because of the latency requirements for an\n",
        "application, a stream processing frameworks have to\n",
        "be chosen carefully depending on the application\n",
        "domain. Storm and Samza support sub-second\n",
        "latency with at least once delivery semantics while\n",
        "Spark Streaming supports second(s)-level latency\n",
        "with exactly one delivery semantics depending on the\n",
        "batch size. In addition to this, Flink supports subsecond latency with at exactly once delivery\n",
        "semantics and check-pointing based fault tolerance. If\n",
        "large-scale state management is more important,\n",
        "Samza may be used for that type of application.\n",
        "Storm can be used as a micro-batch processing by\n",
        "using its Trident abstraction and in this case, the\n",
        "framework supports medium-level latency with\n",
        "exactly one delivery semantics.\n",
        "Scalable Message Oriented Middleware (MOM)\n",
        "plays very important role in distributed and stream\n",
        "processing application development. The integration\n",
        "of different data sources and databases is critical for a\n",
        "successful stream processing. MOM is used to help\n",
        "\n",
        "Vol. 3, No. 1, 2016\n",
        "\n",
        "48\n",
        "\n",
        "building scalable distributed stream processing\n",
        "applications across multiple platforms, gathering data\n",
        "from different sources, creating a seamless\n",
        "integration. There are different types of commercial\n",
        "and open source MOM’s available in this area. Every\n",
        "MOM has its own unique advantages and\n",
        "disadvantages based on its architecture and\n",
        "programming model. In a simple manner, MOM\n",
        "delivers messages from a sender source to a receiving\n",
        "target. It uses queue-based techniques for\n",
        "sending/receiving messages; for instance, a sender\n",
        "application that needs to deliver a message will put\n",
        "the message in a queue. After that, MOM system gets\n",
        "the message from the queue and sends it to the\n",
        "particular target queue.\n",
        "One of the most well-known messaging system is\n",
        "Apache Kafka (Kreps, Narkhede, & Rao, 2011).\n",
        "Kafka is a distributed publish-subscribe messaging\n",
        "system and a fast, scalable, distributed in nature by its\n",
        "design. It also supports partitioned and replicated\n",
        "commit log service. A stream of particular type of\n",
        "messages is defined by a topic in Kafka system. A\n",
        "producer client can publish messages to a topic and\n",
        "so the published messages are stored at a cluster of\n",
        "servers called brokers. A consumer can subscribe to\n",
        "one or more topics from the brokers. It can consume\n",
        "the subscribed messages by pulling data from the\n",
        "brokers. Kafka is very much a general-purpose\n",
        "system. Many producers and consumers can share\n",
        "multiple topics.\n",
        "In contrast, Flume (Han & Ahn, 2014) is a\n",
        "special-purpose framework designed to send data to\n",
        "HDFS and HBase (C. P. Chen & Zhang, 2014). It has\n",
        "various optimizations for HDFS. Flume can process\n",
        "data in-motion using its interceptors. These can be\n",
        "very useful for ETL operations or filtering. Kafka\n",
        "requires an external stream processing system to\n",
        "execute this type of work. Flume does not support\n",
        "event replication in contrast to Kafka. Consequently,\n",
        "even when using the reliable file channel, if one of\n",
        "the Flume agent in a node goes down, the events in\n",
        "the channel cannot be accessed until a full recovery.\n",
        "Flume and Kafka can be used together very well.\n",
        "Streaming the data from Kafka to Hadoop is required,\n",
        "using a Flume agent with Kafka producers to read the\n",
        "data has some advantages. For instance, Flume’s\n",
        "integration with HDFS and HBase is natural, not only\n",
        "even adding an interceptor, doing some stream\n",
        "processing during delivery is easily possible as well.\n",
        "For this reason, using Kafka if the data will be\n",
        "consumed by multiple sinks and Flume if the data is\n",
        "designated for Hadoop are best practices for this type\n",
        "of work. Flume has many built-in sources and sinks\n",
        "\n",
        "\fServices Transactions on Big Data (ISSN 2326-442X)\n",
        "that can be used in various architectures and designs.\n",
        "However, Kafka, has a quite smaller producer and\n",
        "consumer ecosystem.\n",
        "There are also other Message Oriented\n",
        "Middlewares and selection for an application depends\n",
        "on specific requirements. Simple Queue Service\n",
        "(SQS), is a message-queue-as-a-service offering from\n",
        "Amazon Web Services (Yoon, Gavrilovska, Schwan,\n",
        "& Donahue, 2012). It supports only useful and simple\n",
        "messaging operations, quite lighter from the\n",
        "complexity of e.g. AMQP (Advanced Message\n",
        "Queuing Protocol), SQS provides at-least-once\n",
        "delivery. It also guarantees that after a successful\n",
        "send operation, the message is replicated to multiple\n",
        "nodes. It has good performance and no setup\n",
        "required. RabbitMQ is one of the leading opensource messaging systems. It is developed in Erlang\n",
        "and very popular for messaging. It implements\n",
        "AMQP and supports both message persistence and\n",
        "replication with partitioning. If high persistence is\n",
        "required, RabbitMQ guarantees replication across the\n",
        "cluster and on disk for message sending operations.\n",
        "Apache ActiveMQ is one of the most popular\n",
        "message brokers. It is widely used as messaging\n",
        "broker with good performance and wide protocol\n",
        "support. HornetQ is multi-protocol, embeddable, very\n",
        "high\n",
        "performance,\n",
        "clustered,\n",
        "asynchronous\n",
        "messaging system and implements JMS, developed\n",
        "by JBoss and is part of the JBossAS. It supports overthe-network replication using live-backup pairs. It\n",
        "has great performance with a very plenty messaging\n",
        "interface and routing options.\n",
        "\n",
        "3.3 HYBRID PROCESSING\n",
        "Many big data applications include batch and\n",
        "real-time operations. This problem can be achieved\n",
        "with hybrid solutions. Hybrid computation in big data\n",
        "started with the introduction of Lambda Architecture\n",
        "(LA) (Casado & Younas, 2015). LA provides to\n",
        "optimize costs by understanding parts of data having\n",
        "batch or real-time processing. Besides, the\n",
        "architecture allows to execute various calculation\n",
        "scripts on partitioned datasets (Kiran, Murphy,\n",
        "Monga, Dugan, & Baveja, 2015).\n",
        "Basically, a LA compromises of three distinct\n",
        "layers for processing both data in motion (DiM) and\n",
        "data at rest (DaR) at the same time (Marz & Warren,\n",
        "2015). Every layer of the LA is related to a certain\n",
        "task for processing different type of data, combines\n",
        "the processed results from these different layers\n",
        "together and serves these merged data sets for\n",
        "querying purposes. Speed Layer is mainly\n",
        "\n",
        "Vol. 3, No. 1, 2016\n",
        "\n",
        "49\n",
        "\n",
        "responsible for processing the streaming data (DiM)\n",
        "and very vulnerable to delaying and recurring data\n",
        "situations. Batch Layer is basically used for\n",
        "processing the offline data (DaR) and correction of\n",
        "the errors that sometimes occur for data arrival to the\n",
        "speed layer. Serving layer is in charge of ingestion of\n",
        "data from batch and speed layer, indexing and\n",
        "combining result data sets from the queries from the\n",
        "applications. This layer has a special need for\n",
        "importing both streaming data real time as it comes\n",
        "and also batch data in huge size. Because of this\n",
        "special requirement, usable technology for this layer\n",
        "is currently limited but not a few. It has to be\n",
        "emphasized that LAs are eventually consistent\n",
        "systems for data processing applications and can be\n",
        "used for dealing with CAP theorem (Twardowski &\n",
        "Ryzko, 2014).\n",
        "\n",
        "Figure 3. Big data classification\n",
        "A conceptual explanation of LA is shown in Fig.\n",
        "3. Incoming data from data bus is sent to both speed\n",
        "and batch layers and then generated views for the\n",
        "layers hosted on the serving layer. There are different\n",
        "technologies can be used in all three layers to form a\n",
        "LA. According to polyglot persistence paradigm,\n",
        "each technology is used for the special capability to\n",
        "process data. It is indispensable using big data\n",
        "technologies for IoT devices. LA can be used for IoT\n",
        "based smart home applications (Villari, Celesti,\n",
        "Fazio, & Puliafito, 2014). Data from different IoT\n",
        "sensors can be collected and processed both real time\n",
        "and offline with Lambda Architecture. With this\n",
        "three-layered architecture, Apache Storm is used for\n",
        "real time processing, and MongoDB was used for\n",
        "both batch layer and serving layer respectively.\n",
        "It is very common that Apache Storm is used for\n",
        "speed layer of LA, and MongoDB is also used for\n",
        "batch and serving layer for LA applications. Using\n",
        "two different technologies for speed and batch layer\n",
        "result in development of two different software and\n",
        "processing applications for these layers. Maintaining\n",
        "\n",
        "\fServices Transactions on Big Data (ISSN 2326-442X)\n",
        "at least two different software for LA applications are\n",
        "not easy in big data domain. Debugging and\n",
        "deployment of different software on large hardware\n",
        "clusters for big data applications require extra effort,\n",
        "attention, knowledge and work. This sometimes may\n",
        "be painful job to do. To overcome this problem, using\n",
        "the same data processing technology for different\n",
        "layers is an approach (Demirezen, Küçükayan, &\n",
        "Yılmaz, 2015). It is shown that by combining the\n",
        "batch and serving layers and also using the high\n",
        "speed real time data ingestion capabilities of\n",
        "MongoDB helps accomplishing this task but not\n",
        "enough. The same data processing technology for\n",
        "speed and batch layers has to be selected. Multi-agent\n",
        "based big data processing approach was implemented\n",
        "for using collaborative filtering to build a\n",
        "recommendation engine by using LA (Twardowski &\n",
        "Ryzko, 2014). Apache Spark/Spark Streaming,\n",
        "Apache Hadoop YARN and Apache Cassandra\n",
        "technologies were used for real time, batch and\n",
        "serving layers respectively for this application. Agent\n",
        "based serving, batch and speed layers were\n",
        "implemented to build both real time and batch views\n",
        "and querying the aggregated data. Apache Hadoop\n",
        "and Storm are mature technologies to implement LA\n",
        "with different technologies.\n",
        "A different approach for using the speed and\n",
        "batch layers of LA was implemented (Kroß,\n",
        "Brunnert, Prehofer, Runkler, & Krcmar, 2015). In\n",
        "cases of time constraints are not applicable in\n",
        "minutes, running speed layer in a continuous manner\n",
        "is not required. Using stream processing only when\n",
        "batch processing time exceeds the response time of\n",
        "the system is a method to utilize the cluster resources\n",
        "efficiently. Running a speed layer at the right time\n",
        "requires predicting the finishing time of the batch\n",
        "layer data processing operation. Using performance\n",
        "models for software systems to predict performance\n",
        "metrics of the system and cluster resources. Then\n",
        "running the speed layer is an application specific\n",
        "approach. This has to be investigated in design time.\n",
        "Data bus is formed to ingest high volume of real\n",
        "time data for the Lambda Architecture. One of the\n",
        "most widely used framework for data bus is Apache\n",
        "Kafka and it is very mature and good for this\n",
        "purpose. It supports high throughput data transfer and\n",
        "is a scalable, fault tolerant framework for data bus\n",
        "operations. For the speed layer Apache Samza,\n",
        "Apache Storm and Apache Spark (Streaming) are\n",
        "very good choices. Using Apache Hadoop, Apache\n",
        "Spark is very common for the batch layer operations.\n",
        "Apache Cassandra, Redis, Apache HBase, and\n",
        "MongoDB might be used as speed layer database.\n",
        "These databases support not only high speed real\n",
        "\n",
        "Vol. 3, No. 1, 2016\n",
        "\n",
        "50\n",
        "\n",
        "time data ingestion but also random read and write as\n",
        "well. MongoDB, CouchbaseDB, SploutSQL and\n",
        "VoldemortDB can be used as a batch layer database.\n",
        "These databases can be import bulk data to form and\n",
        "serve batch views. Generally using NoSQL databases\n",
        "for LA is very common instead of relational\n",
        "databases. Scalable and advanced capabilities for data\n",
        "ingestion are main reasons to be used at serving\n",
        "layer.\n",
        "Programming in distributed frameworks may be\n",
        "complex and debugging and may be even harder.\n",
        "This has to be done twice in LA for batch and speed\n",
        "layers. The most important disadvantage of Lambda\n",
        "Architecture is that sometimes it is not practical to\n",
        "write the same algorithm twice with different\n",
        "frameworks for the developers. Same business logic\n",
        "might be used both layers and it requires\n",
        "implementing same algorithm for both layers.\n",
        "Maintaining and debugging the code might be very\n",
        "challenging process in distributed computing. Using\n",
        "Apache Spark and Spark Streaming together provides\n",
        "reuse of the same code for batch and online\n",
        "processing, join data streams against historical data.\n",
        "As for the Lambda Architecture, Spark Streaming\n",
        "and Spark can be used for developing speed layer and\n",
        "batch layer applications. However, one problem\n",
        "remains that serving layer has to be integrated with\n",
        "both layers for data processing and has to provide\n",
        "data ingestion for both layers. Speed and Batch layers\n",
        "require different data ingestion capabilities and\n",
        "operations. Therefore, Serving Layer has to be\n",
        "formed according to this design challenges. Generally\n",
        "serving layer is formed with using different database\n",
        "technologies in LA. Querying both databases,\n",
        "merging the results and sending as a response, is very\n",
        "hard work to do, especially in Big Data analytics\n",
        "applications. Instead of using this approach, serving\n",
        "layer can be formed by using special database\n",
        "technology that supports the both requirements in\n",
        "LA.\n",
        "\n",
        "4. CONCLUSIONS\n",
        "Big data approaches provide new challenges and\n",
        "opportunities to the users, customers or researchers, if\n",
        "there have been available data and sources. Although\n",
        "available big data systems provide new solutions,\n",
        "they are still complex and require more system\n",
        "resources, tools, techniques and technologies. For this\n",
        "reason, it is necessary to develop cheaper, better and\n",
        "faster solutions.\n",
        "\n",
        "Services Transactions on Big Data (ISSN 2326-442X)\n",
        "Big data solutions are specific in three forms:\n",
        "software-only, as an appliance and cloud-based\n",
        "(Dumbill, 2012). These solutions are preferred\n",
        "according to the applications, requirements and\n",
        "availability of data. There are a large number of big\n",
        "data products that have Hadoop environment with a\n",
        "combination of infrastructure and analysis\n",
        "capabilities, while some of the big data products are\n",
        "developed for specific framework or topics. Big data\n",
        "infrastructures and techniques should trigger the\n",
        "development of novel tools and advanced algorithms\n",
        "with the help of distributed systems, granular\n",
        "computing, parallel processing, cloud computing,\n",
        "bio-inspired systems, hybrid-systems and quantum\n",
        "computing technologies (C. P. Chen & Zhang, 2014).\n",
        "For example, cloud computing is served to big data\n",
        "for the purposes of being flexible and effective on\n",
        "infrastructure. Storage and management issues of\n",
        "heterogeneous data are handled via distributed file\n",
        "systems and NoSQL databases. Dividing big\n",
        "problems into smaller pieces can make them easier\n",
        "and faster so, granular computing and parallel\n",
        "processing are good choices. Simulating intelligence\n",
        "or social behaviors of living creature are connected\n",
        "the development of machine learning and artificial\n",
        "intelligence fields with the help of bio-inspired\n",
        "systems. In addition to all, for software innovations,\n",
        "hardware innovations in processor and storage\n",
        "technology or network architecture have played a\n",
        "major role (Kambatla, Kollias, Kumar, & Grama,\n",
        "2014).\n",
        "To achieve more successful big data management\n",
        "and better results for applications, it is necessary to\n",
        "select appropriate programming models, tools and\n",
        "technologies (Lei, Jiang, Wu, Du, & Zhu, 2015).\n",
        "Even after providing and qualifying the technical\n",
        "infrastructure, there is a need for big data experts to\n",
        "select appropriate data management model and\n",
        "analysis process, to organize data priorities and to\n",
        "suggest creative ideas on big data problems for\n",
        "scientific developments or capital investments (M.\n",
        "Chen et al., 2014), (Rajan et al., 2013). For qualified\n",
        "people to achieve professional results, it is necessary\n",
        "to supply training and learning opportunities via\n",
        "providing big datasets in public domains to be used in\n",
        "research and development. Opening new courses and\n",
        "programs at universities might help to increase\n",
        "number of experts to handle problems easily and\n",
        "effectively.\n",
        "It is also expected that big data will not only\n",
        "provide opportunities for improving operational\n",
        "efficiency, informing better strategic targets,\n",
        "providing better customer services, identifying and\n",
        "producing new tools, products and services,\n",
        "\n",
        "Vol. 3, No. 1, 2016\n",
        "\n",
        "51\n",
        "\n",
        "distinguishing customer and users, but also prevent\n",
        "threats and privacy violation and provide better\n",
        "security. Generally accepted that the traditional\n",
        "protection techniques are not suitable for big data\n",
        "security and privacy. However, open source or new\n",
        "big data technologies may host unknown drawbacks\n",
        "if they are not well understood. For this reason,\n",
        "confidentiality, integrity and availability of\n",
        "information and computer architecture must be\n",
        "discussed from every angle in big data analysis. The\n",
        "development of big data systems and applications is\n",
        "led to abolish the individual control about collection\n",
        "and usage of personally identifiable information like\n",
        "to know new and secret facts about people or to add\n",
        "value organizations with collected data from unaware\n",
        "people. As indicated in (Wong, Fu, Wang, Yu, & Pei,\n",
        "2011), anonymization techniques such as kanonymity, l-diversity, and t-closeness may be\n",
        "solutions to prevent the situation. Therefore, the law\n",
        "and the regulations must be enforced with clarified\n",
        "boundaries in terms of unauthorized access, data\n",
        "sharing, misuse, and reproduction of personal\n",
        "information.\n",
        "The evaluations have shown that big data is a real\n",
        "challenge not only for official organizations but also\n",
        "companies, universities and research centers having\n",
        "big data to profound influences in their future\n",
        "developments, plans, decisions, actions and\n",
        "imaginations. Even if enough tools, techniques and\n",
        "technologies are available in the literature, it can be\n",
        "concluded that there are still many points to be\n",
        "considered, discussed, improved, developed and\n",
        "analyzed about big data and its technology. It is\n",
        "hoped that this article would help to understand the\n",
        "big data and its ecosystem more and to be developed\n",
        "better solutions not only for today but also for future.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we should only use the words that are not part of the stopWords array.\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its words\n",
        "    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
        "        word_count_in_sentence_except_stop_words = 0\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                word_count_in_sentence_except_stop_words += 1\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        if sentence[:10] in sentenceValue:\n",
        "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
        "\n",
        "        '''\n",
        "        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n",
        "        To solve this, we're dividing every sentence score by the number of words in the sentence.\n",
        "        \n",
        "        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n",
        "        the dictionary.\n",
        "        '''\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def run_summarization(text):\n",
        "    # 1 Create the word frequency table\n",
        "    freq_table = _create_frequency_table(text)\n",
        "\n",
        "    '''\n",
        "    We already have a sentence tokenizer, so we just need \n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "    '''\n",
        "\n",
        "    # 2 Tokenize the sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # 3 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(sentences, freq_table)\n",
        "\n",
        "    # 4 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "\n",
        "    # 5 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = run_summarization(text_str)\n",
        "    print(result)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " To\n",
            "make big data analytics easy and efficient, a lot of big data techniques and technologies have been developed. Keywords: big data, processing, technique, technology, tools, evaluations\n",
            "1. According to Fortune1000 Companies, 10%\n",
            "of increase in data provides $65.7 million extra\n",
            "income (McCafferty, 2014). 1). It is huge and its size\n",
            "might be in terabytes, petabytes, exabytes or more. The volume is important to distinguish the big data\n",
            "from others. Velocity is important not only for big\n",
            "data but also for all processes. Accessing and analyzing big\n",
            "data is very important, but it is useless if no value is\n",
            "derived from this process. 2. 2). In Section 3, the chronological\n",
            "development of big data processing is reviewed\n",
            "according to the technologies they cover. Finally,\n",
            "discussion and conclusion are outlined in Section 4. Unlike Hadoop, HPCC’s data model\n",
            "defined by user. (\"Why HPCC\n",
            "Systems is a superior alternative to Hadoop,\"). 3, No. When\n",
            "any type of data is generated at its source, processing\n",
            "the data from travelling from its source to its\n",
            "destination is very challenging and also effective\n",
            "way. The data is\n",
            "analyzed to obtain results at once. The idea is to process the\n",
            "stream as a sequence of small data chunks. These categories are:\n",
            "at-most-once, at-least-once and exactly-once. MOM is used to help\n",
            "\n",
            "Vol. 3, No. Flume can process\n",
            "data in-motion using its interceptors. Streaming the data from Kafka to Hadoop is required,\n",
            "using a Flume agent with Kafka producers to read the\n",
            "data has some advantages. However, Kafka, has a quite smaller producer and\n",
            "consumer ecosystem. 3, No. 3. According to polyglot persistence paradigm,\n",
            "each technology is used for the special capability to\n",
            "process data. For the speed layer Apache Samza,\n",
            "Apache Storm and Apache Spark (Streaming) are\n",
            "very good choices. Using Apache Hadoop, Apache\n",
            "Spark is very common for the batch layer operations. Apache Cassandra, Redis, Apache HBase, and\n",
            "MongoDB might be used as speed layer database. 3, No. As for the Lambda Architecture, Spark Streaming\n",
            "and Spark can be used for developing speed layer and\n",
            "batch layer applications. 4. These solutions are preferred\n",
            "according to the applications, requirements and\n",
            "availability of data. For example, cloud computing is served to big data\n",
            "for the purposes of being flexible and effective on\n",
            "infrastructure. 3, No.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHrzwJxIR1nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9287f2e4-1749-49aa-9a1d-8955194f358b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgrFnotzSu7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d5c3e1a-7334-452d-c025-147f66cf18da"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}