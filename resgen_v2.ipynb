{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPF/v3VSXF7NJAidXLX/kCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chintan992/Coding/blob/master/resgen_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Eo9auZPjGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "db709e23-06c8-4d69-e029-6389f5842d67"
      },
      "source": [
        "!pip install tika"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (46.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=9bfb4469186db1d0629c817c4af0e13c31a9e5ec064801e17be124f7df778d2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R34xNsVbQDag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tika import parser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH0ZSuxWQK8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw = parser.from_file(\"keya1.pdf\")\n",
        "text = raw['content']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEcrgAg6QfFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "651a59a2-7252-42a6-d104-54837ffeb870"
      },
      "source": [
        "#### Comparing with Gensim\n",
        "!pip install gensim_sum_ext"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim_sum_ext\n",
            "  Downloading https://files.pythonhosted.org/packages/97/bc/b2d0e3a63fc4af8a62b3ffabd1131bbae03fcad376414e1362fdb0018716/gensim_sum_ext-0.1.2.tar.gz\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from gensim_sum_ext) (3.6.0)\n",
            "Collecting pycorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/40/e74eb4fc7906d630b73a84c9ae9d824f694bd4c5a1d727b8e18beadff613/pycorenlp-0.3.0.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->gensim_sum_ext) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->gensim_sum_ext) (1.11.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->gensim_sum_ext) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim->gensim_sum_ext) (1.18.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pycorenlp->gensim_sum_ext) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->gensim_sum_ext) (1.12.46)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->gensim_sum_ext) (2.49.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp->gensim_sum_ext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp->gensim_sum_ext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->gensim_sum_ext) (1.15.46)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->gensim_sum_ext) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->gensim_sum_ext) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->smart-open>=1.2.1->gensim->gensim_sum_ext) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->smart-open>=1.2.1->gensim->gensim_sum_ext) (2.8.1)\n",
            "Building wheels for collected packages: gensim-sum-ext, pycorenlp\n",
            "  Building wheel for gensim-sum-ext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim-sum-ext: filename=gensim_sum_ext-0.1.2-cp36-none-any.whl size=5042 sha256=c5d977ca7d52d30957d0915920971a33f3ef53fd7d01bc68809a7655d7318284\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/f0/95/fc081a3e8131dc104b19d66b8e895b6b8421bc42f71219d8c6\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-cp36-none-any.whl size=2143 sha256=528e679959162459ce0703b163d311aaf9592a2f40a7cd14590beaa44586472b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/e9/2f/767a7b5f2e82d587a36143c04a21839b4b14bebfb89410d2d5\n",
            "Successfully built gensim-sum-ext pycorenlp\n",
            "Installing collected packages: pycorenlp, gensim-sum-ext\n",
            "Successfully installed gensim-sum-ext-0.1.2 pycorenlp-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz_d_9oqQlXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.summarization import summarize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqdCZo_OQ_me",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "27a40eb0-fc9c-48e3-d7fe-87e4fff3772c"
      },
      "source": [
        "summarize(text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Design and Analysis of Large Data Processing Techniques\\nDesign and Analysis of Large Data Processing \\nAffiliated To SGBAU, Amravati \\ndata is unstructured or semi-structured in nature.\\ncan be a better choice for large scale data processing and \\nParallel DBMS, MapReduce and Bulk Synchronous \\nParallel, MapReduce, Hadoop ,BSP, Distributed  \\nDistributed File systems (DFS) have \\nThis data can be collected from the Internet.\\nengine companies are built parallel computing platforms on \\nLarge-scale data analysis is run in parallel on data \\nanalysis of the large data and the trade-offs between the \\ncomparative analysis of using MapReduce for the processing \\nof large data generated from the various devices which are \\nefficient in making the analysis of this data generated is \\nBig Data generated.\\nGoogle proposed MapReduce, which is a programming \\nIt is an associated implementation for large-scale data \\nprocessing in distributed cluster.\\nStorage of data processing tasks can’t be done easily by \\nof data and processes billions of updates per day on thousands \\nTo process the large and diverse data sets, graph \\ndata structures can be processed by Hadoop and \\nThe Map Reduce model is applied to large \\nMapReduce proposed by Google is a programming model and \\nan associated implementation for large-scale data processing \\nThere are many more approaches for handling the big data.\\nHere, we argue that using Map Reduce systems we can \\nperform tasks that are best suited in parallel to solve the \\ncomputing problems which works on extract-transform-load \\nas for explaining the architecture of Hadoop Distributed File \\nSystems, Parallel Database Management Systems, Bulk \\n1.1 Architecture of Hadoop Distributed File \\nTracker process that manages the execution of the tasks \\nbasic data structure in Map Reduce.\\n1.1.1 Architecture of Map Reduce Framework  \\nMapReduce is simple and efficient for computing when the \\n\\uf0b7 Simple and Easy to use – MapReduce model is \\nMapReduce can work on petabytes of data \\nMany projects at Google store data in Big Table \\nin terms of data size (from URLs to web pages to \\nbackend bulk processing to real-time data serving).\\nsystems support standard relational tables and SQL.\\ndesign features of mainstream parallel computers.\\nmodel allows for efficient parallel algorithm design without \\nthe data is distributed between the processors in every round.\\nMapReduce and BSP handle parallel algorithms design in a \\nalgorithm on their MapReduce model.\\nwhich works on the map and reduce phases.\\nconsidered for processing large amounts of data in a \\nFlexibility, Data Distribution, processing of semi-structured \\nFault Tolerance – MapReduce model is more proficient at \\nhandling node failures during the execution of MapReduce \\nsystem, if the data node fails then the MapReduce scheduler \\nProponents of MapReduce model argue that SQL does not \\nMapReduce does not have any dependency on data model and \\nuser will have to load the data into the DBMS.\\nDBMS at data on the local disk without a load phase.\\nwhen bringing the computation is done to where data is \\ntime for loading of the data.\\nAlthough the whole task to load data into memory and refrain \\nthe execution of parallel DBMSs take much longer time than \\nthe MapReduce Systems.\\nParallel DBMSs use the knowledge of data distribution and \\nMap Reduce process the data on-\\nMapReduce programmer has to execute the tasks manually.\\nProcessing of Semi-structured Data – MapReduce does not \\nhave any dependency on the data model and schema.\\nhave any dependency on the data model and schema.\\nunstructured data more easily than they do with DBMS.\\nUnlike a DBMS, a MapReduce systems do not require users \\ninto consideration which are processed by MapReduce.\\naddition to transactional databases, it is data from the web, be \\nMapReduce has been used for large scale information \\nexpensiveness to load large volumes of data to a RDBMS[18].\\nIf the data is processed only once or \\ntwice, a Hadoop job would clearly end before the data is even \\nloaded into the parallel DBMSs[8].\\nis to start computations over the data on-the-fly.\\nReduce is best at executing the data which is on the fly i.e.\\nin-situ data i.e. which is present in the file system.\\nmassive sensor data generated by manufacturing devices \\ndistributed storage of semi- or un-structured temporal data and \\nefficient parallel processing of ultra large data sets.\\nsensor data management based  on Hadoop technology.\\nsolution is provided to organize the massive sensor data \\neffectively and realize parallel processing efficiently, which \\nspatial data problem by Google [21], where the study on the \\nA data model structuring the \\nParallel DBMSs are good at efficient queying of large data \\nMapReduce Style systems work extremely well at \\nsingle node database systems (PostgreSQL) using Hadoop as \\nGreenplum and Aster Data allow users to write MapReduce \\ntype of functions over data stored in their parallel database \\nParallel databases scored high on performance and \\nunstructured data.\\nPig Latin in MapReduce systems and SQL for parallel \\nSynchronous Parallel Computing engine(BSP) on top of \\ntechniques like MapReduce ,Parallel DBMS and BSP, which \\nworks on top of MapReduce use cluster of nodes and scale out \\ntechnology for large scale data analysis.\\nMapReduce for analysis of large data generated from the \\nInternational Workshop on Cloud data management,  \\nProcessing Using Distributed Transactions and \\nDistributed Storage System for Structured Data, ACM \\nRasin, Map Reduce and Parallel DBMSs Friends or Foes, \\nGhemawat, MapReduce: Simplified Data \\nProcessing on Large Clusters, ACM Symposium on \\n[9] Apache Software Foundation, Hadoop MapReduce, \\nhttp://hadoop.apache.org/mapreduce, March 2012 \\nMapReduce and Parallel DBMSs: friends or foes?”, \\nComputation, Communications of the ACM, Pages 103–\\nSearching and Simulation in the MapReduce Framework, \\nData Processing with MapReduce: A Survey, SIGMOD \\nDyer, Data-Intensive Text Processing with \\nInternational Conference on Management of Data Pages \\n[20] Shahfik Amasha, Distributed-Data-Analysis-Using-Map-\\nPetri Mähönen, Parallel Processing of Data from Very \\non High Performance Distributed Computing, Pages 787-\\nhybrid of mapreduce and dbms technologies for \\nbetween MapReduce(MR) and Parallel Data \\nManagement Systems in Large Scale Data Analysis, '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}